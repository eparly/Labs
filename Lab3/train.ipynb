{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Ar7cVcDc8wXG",
        "outputId": "3e7a4324-02e3-4877-e61f-2bfa9ca239f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:05<00:00, 28.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Training VGG16\n",
            "Using device:  cuda\n",
            "batch 0 of 625\n",
            "batch 1 of 625\n",
            "batch 2 of 625\n",
            "batch 3 of 625\n",
            "batch 4 of 625\n",
            "batch 5 of 625\n",
            "batch 6 of 625\n",
            "batch 7 of 625\n",
            "batch 8 of 625\n",
            "batch 9 of 625\n",
            "batch 10 of 625\n",
            "batch 11 of 625\n",
            "batch 12 of 625\n",
            "batch 13 of 625\n",
            "batch 14 of 625\n",
            "batch 15 of 625\n",
            "batch 16 of 625\n",
            "batch 17 of 625\n",
            "batch 18 of 625\n",
            "batch 19 of 625\n",
            "batch 20 of 625\n",
            "batch 21 of 625\n",
            "batch 22 of 625\n",
            "batch 23 of 625\n",
            "batch 24 of 625\n",
            "batch 25 of 625\n",
            "batch 26 of 625\n",
            "batch 27 of 625\n",
            "batch 28 of 625\n",
            "batch 29 of 625\n",
            "batch 30 of 625\n",
            "batch 31 of 625\n",
            "batch 32 of 625\n",
            "batch 33 of 625\n",
            "batch 34 of 625\n",
            "batch 35 of 625\n",
            "batch 36 of 625\n",
            "batch 37 of 625\n",
            "batch 38 of 625\n",
            "batch 39 of 625\n",
            "batch 40 of 625\n",
            "batch 41 of 625\n",
            "batch 42 of 625\n",
            "batch 43 of 625\n",
            "batch 44 of 625\n",
            "batch 45 of 625\n",
            "batch 46 of 625\n",
            "batch 47 of 625\n",
            "batch 48 of 625\n",
            "batch 49 of 625\n",
            "batch 50 of 625\n",
            "batch 51 of 625\n",
            "batch 52 of 625\n",
            "batch 53 of 625\n",
            "batch 54 of 625\n",
            "batch 55 of 625\n",
            "batch 56 of 625\n",
            "batch 57 of 625\n",
            "batch 58 of 625\n",
            "batch 59 of 625\n",
            "batch 60 of 625\n",
            "batch 61 of 625\n",
            "batch 62 of 625\n",
            "batch 63 of 625\n",
            "batch 64 of 625\n",
            "batch 65 of 625\n",
            "batch 66 of 625\n",
            "batch 67 of 625\n",
            "batch 68 of 625\n",
            "batch 69 of 625\n",
            "batch 70 of 625\n",
            "batch 71 of 625\n",
            "batch 72 of 625\n",
            "batch 73 of 625\n",
            "batch 74 of 625\n",
            "batch 75 of 625\n",
            "batch 76 of 625\n",
            "batch 77 of 625\n",
            "batch 78 of 625\n",
            "batch 79 of 625\n",
            "batch 80 of 625\n",
            "batch 81 of 625\n",
            "batch 82 of 625\n",
            "batch 83 of 625\n",
            "batch 84 of 625\n",
            "batch 85 of 625\n",
            "batch 86 of 625\n",
            "batch 87 of 625\n",
            "batch 88 of 625\n",
            "batch 89 of 625\n",
            "batch 90 of 625\n",
            "batch 91 of 625\n",
            "batch 92 of 625\n",
            "batch 93 of 625\n",
            "batch 94 of 625\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f11488ac1ee2>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using device: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{model_name}.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{model_name}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Test both checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f11488ac1ee2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, val_loader, num_epochs, checkpoint_epochs, save_file, plot_file)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"batch {n} of {len(train_loader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mn\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CIFAR100 Dataset with fine labels\n",
        "transform = transforms.Compose([transforms.Resize(224), transforms.ToTensor(), transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))])\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split train into training and validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_data, val_data = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=256, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=256, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Define models\n",
        "models_dict = {\n",
        "    # 'AlexNet': models.alexnet(),\n",
        "    'VGG16': models.vgg16(),\n",
        "    'ResNet18': models.resnet18()\n",
        "}\n",
        "\n",
        "# Modify the classifiers to output 100 classes for CIFAR100\n",
        "for model_name, model in models_dict.items():\n",
        "    if model_name == 'AlexNet' or model_name == 'VGG16':\n",
        "        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 100)\n",
        "    elif model_name == 'ResNet18':\n",
        "        model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "\n",
        "# Training function\n",
        "def train(model, device, train_loader, val_loader, num_epochs, checkpoint_epochs, save_file=None, plot_file=None):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        n=0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            print(f\"batch {n} of {len(train_loader)}\")\n",
        "            n+=1\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        print(datetime.datetime.now(), 'epoch:', epoch, 'train loss:', avg_loss)\n",
        "\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        val_avg_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_avg_loss)\n",
        "\n",
        "        print(datetime.datetime.now(), 'epoch:', epoch, 'val loss:', val_avg_loss)\n",
        "\n",
        "\n",
        "        if save_file != None:\n",
        "            torch.save(model.state_dict(), save_file)\n",
        "\n",
        "        if plot_file != None:\n",
        "            plt.figure(2, figsize=(12, 7))\n",
        "            plt.clf()\n",
        "            plt.plot(train_losses, label='train')\n",
        "            plt.plot(val_losses, label='val')\n",
        "            plt.xlabel('epoch')\n",
        "            plt.ylabel('loss')\n",
        "            plt.legend(loc=1)\n",
        "            print('saving ', plot_file)\n",
        "            plt.savefig(plot_file)\n",
        "\n",
        "        # Save checkpoint if at a checkpoint epoch\n",
        "        if epoch + 1 in checkpoint_epochs:\n",
        "            torch.save(model.state_dict(), f\"{model_name}_epoch_{epoch+1}.pth\")\n",
        "\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Testing function\n",
        "def test(model, test_loader, top_k):\n",
        "    model.eval()\n",
        "    top1_errors, top5_errors = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, pred_top5 = outputs.topk(5, 1, largest=True, sorted=True)\n",
        "            top1_errors += (pred_top5[:, 0] != labels).sum().item()\n",
        "            top5_errors += (pred_top5 == labels.view(-1, 1)).sum().item()\n",
        "\n",
        "    total_samples = len(test_loader.dataset)\n",
        "    top1_error_rate = top1_errors / total_samples\n",
        "    top5_error_rate = (total_samples - top5_errors) / total_samples\n",
        "    return top1_error_rate, top5_error_rate\n",
        "\n",
        "# Run the process for each model\n",
        "checkpoint_epochs = [5, 50]  # Modify '50' to the desired full convergence epoch count\n",
        "results = {}\n",
        "for model_name, model in models_dict.items():\n",
        "    print(f\"Training {model_name}\")\n",
        "    print('Using device: ', device)\n",
        "    model = model.to(device)\n",
        "    train_losses, val_losses = train(model, device, train_loader, val_loader, num_epochs=50, checkpoint_epochs=checkpoint_epochs, save_file=f\"{model_name}.pth\", plot_file=f\"{model_name}.png\")\n",
        "\n",
        "    # Test both checkpoints\n",
        "    for epoch in checkpoint_epochs:\n",
        "        model.load_state_dict(torch.load(f\"{model_name}_epoch_{epoch}.pth\"))\n",
        "        top1, top5 = test(model, test_loader, top_k=5)\n",
        "        results[f\"{model_name}_epoch_{epoch}\"] = {\"top1_error\": top1, \"top5_error\": top5}\n",
        "        print(f\"{model_name} Epoch {epoch} - Top-1 Error: {top1}, Top-5 Error: {top5}\")\n",
        "\n",
        "# Save or print final results\n",
        "print(\"Final results:\", results)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}